{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11920075,"sourceType":"datasetVersion","datasetId":7493905},{"sourceId":409037,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":334212,"modelId":355201}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model 1 Capstone Project : Klasifikasi Gambar Menggunakan Convolutional Neural Networks (CNN)\nNotebook ini bertujuan untuk membangun dan melatih model **Convolutional Neural Network (CNN)** untuk melakukan **klasifikasi gambar organik dan anorganik**. Langkah-langkah yang dilakukan meliputi persiapan data, pembuatan model, pelatihan, dan evaluasi model. Model CNN yang dibangun akan digunakan untuk mengklasifikasikan gambar sampah rumah tangga ke dalam dua kategori utama: **organik** dan **anorganik**.\n\n# Nama Proyek : WasteSnap\nProyek inovatif berbasis website yang bertujuan untuk membantu masyarakat Indonesia mengidentifikasi dan mengelola sampah rumah tangga secara mandiri melalui platform berbasis web.\n\n# Tujuan Proyek:\n- **Mengidentifikasi Sampah Organik dan Non-Organik**: Dengan menggunakan model CNN yang dilatih pada dataset gambar, pengguna dapat mengidentifikasi jenis sampah secara otomatis.\n- **Meningkatkan Kesadaran Lingkungan**: Proyek ini bertujuan untuk meningkatkan kesadaran masyarakat mengenai pengelolaan sampah dan pentingnya pemisahan sampah organik dan non-organik untuk mendukung program daur ulang.\n- **Pengelolaan Sampah yang Lebih Efisien**: Dengan aplikasi ini, masyarakat dapat memisahkan sampah mereka dengan lebih mudah dan efisien, serta membantu mengurangi dampak lingkungan yang disebabkan oleh pembuangan sampah sembarangan.\n\nMelalui proyek ini, teknologi klasifikasi gambar diharapkan dapat membantu masyarakat Indonesia dalam pengelolaan sampah rumah tangga secara lebih mandiri dan ramah lingkungan.\n\n# Anggota Tim Machine Learning\n## ID tim : CC25-CF318. \nArthur Setiawan Waruwu | MC319D5Y2042 | Universitas Sumatera Utara | \\\nSakifa Indira Putri | MC319D5X2380 | Universitas Sumatera Utara | \\\nDiva Anggreini Harahap | MC319D5X2329 | Universitas Sumatera Utara |\n\n# Penjelasan Dataset\nDataset ini berisi gambar-gambar sampah rumah tangga yang sudah kami kelompokkan dalam folder yang sesuai dengan jenisnya. Dataset ini memiliki 13.8k gambar, yang masing-masing kami kumpulkan secara mandiri dari berbagai sumber, seperti Kaggle, Google Images, Pinterest, dan sumber-sumber lainnya. Berikut adalah kategori-kategori yang terdapat dalam dataset ini:\n- Buah\n- Cangkang Telur\n- Elektronik\n- Kaca\n- Kain\n- Kardus\n- Karet\n- Kayu\n- Kertas\n- Kotoran Hewan\n- Logam\n- Plastik\n- Sayuran\n- Sepatu\n- Sisa Teh dan Kopi\n- Sisa Makanan\n- Styrofoam","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Libraries\nLangkah pertama yang akan kita lakukan adalah mengimpor berbagai libraries yang menyediakan fungsi-fungsi yang kita perlukan.","metadata":{}},{"cell_type":"code","source":"# Install tensorflow menggunakan perintah pip\n!pip install tensorflow opencv-python Pillow tensorflowjs","metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-05-23T17:54:55.896809Z","iopub.execute_input":"2025-05-23T17:54:55.897100Z","iopub.status.idle":"2025-05-23T17:54:59.349078Z","shell.execute_reply.started":"2025-05-23T17:54:55.897080Z","shell.execute_reply":"2025-05-23T17:54:59.348105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Library yang diperlukan\n# Untuk pengolahan citra\nimport cv2\nfrom PIL import Image, ImageEnhance  \n\n# Untuk operasi array dan manipulasi data\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport shutil\n\n# Untuk visualisasi\nimport matplotlib.pyplot as plt  # Matplotlib\n\n# Untuk machine learning / deep learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input,Conv2D, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping,  ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split  \nimport tensorflowjs as tfjs\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:54:59.350710Z","iopub.execute_input":"2025-05-23T17:54:59.351019Z","iopub.status.idle":"2025-05-23T17:54:59.357829Z","shell.execute_reply.started":"2025-05-23T17:54:59.350995Z","shell.execute_reply":"2025-05-23T17:54:59.357165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Preparation\nPersiapan data untuk klasifikasi gambar melibatkan pengolahan gambar sehingga dapat dimasukkan ke dalam model CNN.","metadata":{}},{"cell_type":"markdown","source":"## Data Loading\nPada bagian ini, kita akan memuat dataset gambar yang akan digunakan untuk train dan validation model. Dataset telah diupload ke kaggle dan akan dimuat langsung ke dalam notebook. Sebelum itu, kita akan menyalin dataset dari direktori `/kaggle/input/` yang sifatnya itu read-only ke direktori working `/kaggle/working/` yang dapat ditulis, jadi kita bisa melakukan berbagai operasi tanpa terbatas oleh izin akses read-only.","metadata":{}},{"cell_type":"code","source":"dataset_dir = '/kaggle/input/datasetcapstonefixx/Dataset-Asli'\n\nworking_dir = '/kaggle/working/dataset-capstone'\n\nif not os.path.exists(dataset_dir):\n    print(f\"Direktori sumber {dataset_dir} tidak ditemukan!\")\nelse:\n    if os.path.exists(working_dir):\n        print(f\"Folder {working_dir} sudah ada, sedang dihapus...\")\n        shutil.rmtree(working_dir) \n\n    # menyalin dataset dari /kaggle/input ke /kaggle/working yang bisa ditulis\n    try:\n        shutil.copytree(dataset_dir, working_dir)\n        print(f\"Dataset berhasil disalin ke {working_dir}\")\n    except Exception as e:\n        print(f\"Gagal menyalin dataset: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:54:59.358502Z","iopub.execute_input":"2025-05-23T17:54:59.358682Z","iopub.status.idle":"2025-05-23T17:55:25.168085Z","shell.execute_reply.started":"2025-05-23T17:54:59.358669Z","shell.execute_reply":"2025-05-23T17:55:25.167380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# menghitung total gambar di dataset\ndef count_images_per_class_in_main_folders(base_dir, main_folders):\n    total_images = 0  \n    \n    for main_folder in main_folders:\n        main_path = os.path.join(base_dir, main_folder)\n        if not os.path.isdir(main_path):\n            print(f\"Folder {main_folder} tidak ditemukan di {base_dir}\")\n            continue\n        \n        kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n        print(f\"\\nFolder utama '{main_folder}' memiliki {len(kelas_list)} kelas:\")\n        \n        for kelas in kelas_list:\n            kelas_path = os.path.join(main_path, kelas)\n            images = [f for f in os.listdir(kelas_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n            jumlah = len(images)\n            print(f\"  Kelas '{kelas}' : {jumlah} gambar\")\n            total_images += jumlah\n    \n    print(f\"\\nTotal seluruh gambar di semua kelas: {total_images}\")\n\nmain_folders = ['Anorganik', 'Organik']\ncount_images_per_class_in_main_folders(working_dir, main_folders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:25.169707Z","iopub.execute_input":"2025-05-23T17:55:25.169937Z","iopub.status.idle":"2025-05-23T17:55:25.187380Z","shell.execute_reply.started":"2025-05-23T17:55:25.169918Z","shell.execute_reply":"2025-05-23T17:55:25.186765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# memeriksa ekstensi gambar yang valid\nvalid_extensions = {'.jpg', '.jpeg', '.png'}\n\ndef cek_ekstensi_tidak_valid(base_dir, main_folders):\n    invalid_files = []\n    \n    for main_folder in main_folders:\n        main_path = os.path.join(base_dir, main_folder)\n        if not os.path.isdir(main_path):\n            print(f\"Folder {main_folder} tidak ditemukan di {base_dir}\")\n            continue\n        \n        kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n        \n        for kelas in kelas_list:\n            kelas_path = os.path.join(main_path, kelas)\n            files = os.listdir(kelas_path)\n            \n            for f in files:\n                ext = os.path.splitext(f)[1].lower()\n                if ext not in valid_extensions:\n                    invalid_files.append(os.path.join(main_folder, kelas, f))\n    \n    if invalid_files:\n        print(\"File dengan ekstensi tidak valid ditemukan:\")\n        for file_path in invalid_files:\n            print(f\" - {file_path}\")\n        print(f\"\\nTotal file dengan ekstensi tidak valid: {len(invalid_files)}\")\n    else:\n        print(\"Semua file memiliki ekstensi gambar yang valid.\")\n        print(\"Total file tidak valid: 0\")\n\n# Contoh pemanggilan fungsi\nmain_folders = ['Anorganik', 'Organik']\ncek_ekstensi_tidak_valid(working_dir, main_folders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:25.188007Z","iopub.execute_input":"2025-05-23T17:55:25.188207Z","iopub.status.idle":"2025-05-23T17:55:25.210815Z","shell.execute_reply.started":"2025-05-23T17:55:25.188193Z","shell.execute_reply":"2025-05-23T17:55:25.210199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Melihat dari output di atas, tampaknya masih ada sedikit gambar dengan ekstensi .webp yang tidak valid. Oleh karena itu, kita akan menghapus gambar-gambar ini terlebih dahulu sebelum membagi dataset.","metadata":{}},{"cell_type":"code","source":"# menghapus gambar yang tidak valid\ndef hapus_file_tidak_valid(base_dir, main_folders):\n    deleted_files = []\n    \n    for main_folder in main_folders:\n        main_path = os.path.join(base_dir, main_folder)\n        if not os.path.isdir(main_path):\n            print(f\"Folder {main_folder} tidak ditemukan di {base_dir}\")\n            continue\n        \n        kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n        \n        for kelas in kelas_list:\n            kelas_path = os.path.join(main_path, kelas)\n            files = os.listdir(kelas_path)\n            \n            for f in files:\n                ext = os.path.splitext(f)[1].lower()\n                if ext not in valid_extensions:\n                    file_path = os.path.join(kelas_path, f)\n                    try:\n                        os.remove(file_path)\n                        deleted_files.append(os.path.join(main_folder, kelas, f))\n                    except Exception as e:\n                        print(f\"Gagal menghapus {file_path}: {e}\")\n    \n    if deleted_files:\n        print(\"Berhasil menghapus file berikut dengan ekstensi tidak valid:\")\n        for file_path in deleted_files:\n            print(f\" - {file_path}\")\n        print(f\"\\nTotal file yang dihapus: {len(deleted_files)}\")\n    else:\n        print(\"Tidak ditemukan file dengan ekstensi tidak valid untuk dihapus.\")\n\nmain_folders = ['Anorganik', 'Organik']\nhapus_file_tidak_valid(working_dir, main_folders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:25.211477Z","iopub.execute_input":"2025-05-23T17:55:25.211641Z","iopub.status.idle":"2025-05-23T17:55:25.238865Z","shell.execute_reply.started":"2025-05-23T17:55:25.211628Z","shell.execute_reply":"2025-05-23T17:55:25.238319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Pada tahap ini, telah dilakukan pengecekan dan penghapusan terhadap gambar-gambar yang tidak valid dalam dataset. Berdasarkan hasil pemeriksaan, sebanyak **34 gambar** dengan ekstensi tidak valid telah dihapus.","metadata":{}},{"cell_type":"code","source":"def cek_ekstensi_tidak_valid(base_dir, main_folders):\n    invalid_files = []\n    \n    for main_folder in main_folders:\n        main_path = os.path.join(base_dir, main_folder)\n        if not os.path.isdir(main_path):\n            continue\n        \n        kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n        \n        for kelas in kelas_list:\n            kelas_path = os.path.join(main_path, kelas)\n            files = os.listdir(kelas_path)\n            \n            for f in files:\n                ext = os.path.splitext(f)[1].lower()\n                if ext not in valid_extensions:\n                    invalid_files.append(os.path.join(main_folder, kelas, f))\n    \n    if invalid_files:\n        print(\"Masih terdapat file dengan ekstensi tidak valid:\")\n        for file_path in invalid_files:\n            print(f\" - {file_path}\")\n    else:\n        print(\"Tidak ditemukan file dengan ekstensi tidak valid lagi.\")\n\nmain_folders = ['Anorganik', 'Organik']\ncek_ekstensi_tidak_valid(working_dir, main_folders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:25.239637Z","iopub.execute_input":"2025-05-23T17:55:25.239866Z","iopub.status.idle":"2025-05-23T17:55:25.262357Z","shell.execute_reply.started":"2025-05-23T17:55:25.239844Z","shell.execute_reply":"2025-05-23T17:55:25.261779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Nah, bisa dipastikan bahwa seluruh gambar dalam dataset ini sudah **valid** dan siap digunakan untuk pelatihan model.","metadata":{}},{"cell_type":"code","source":"def show_sample_images(base_dir, main_folders):\n    for main_folder in main_folders:\n        main_path = os.path.join(base_dir, main_folder)\n        if not os.path.isdir(main_path):\n            print(f\"Folder {main_folder} tidak ditemukan.\")\n            continue\n        \n        print(f\"\\n--- Contoh gambar dari kelas di folder '{main_folder}' ---\")\n        kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n        \n        images = []\n        titles = []\n        \n        for kelas in kelas_list:\n            kelas_path = os.path.join(main_path, kelas)\n            files = [f for f in os.listdir(kelas_path) if os.path.splitext(f)[1].lower() in valid_extensions]\n            \n            if len(files) == 0:\n                print(f\"Tidak ada gambar valid di kelas '{kelas}'\")\n                continue\n            \n            contoh_gambar_path = os.path.join(kelas_path, files[0])\n            img = cv2.imread(contoh_gambar_path)\n            if img is None:\n                print(f\"Gagal membaca gambar {contoh_gambar_path}\")\n                continue\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            images.append(img)\n            titles.append(f\"{main_folder} - {kelas}\")\n        \n        if len(images) == 0:\n            print(f\"Tidak ada gambar valid untuk ditampilkan di folder '{main_folder}'.\")\n            continue\n        \n        cols = 4\n        total = len(images)\n        rows = (total + cols - 1) // cols  \n\n        fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5)) \n        axes = axes.flatten() if total > 1 else [axes]\n\n        for i in range(len(axes)):\n            if i < total:\n                axes[i].imshow(images[i])\n                axes[i].set_title(titles[i], fontsize=14)\n                axes[i].axis('off')\n            else:\n                axes[i].axis('off') \n        \n        plt.tight_layout()\n        plt.show()\n\n# Contoh pemanggilan fungsi\nmain_folders = ['Anorganik', 'Organik']\nshow_sample_images(working_dir, main_folders)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:25.263019Z","iopub.execute_input":"2025-05-23T17:55:25.263237Z","iopub.status.idle":"2025-05-23T17:55:28.090523Z","shell.execute_reply.started":"2025-05-23T17:55:25.263222Z","shell.execute_reply":"2025-05-23T17:55:28.089364Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split Dataset","metadata":{}},{"cell_type":"code","source":"# split data 80% Train, 10% Test, 10% Val\n\nworking_dir = '/kaggle/working/dataset-capstone'  \noutput_dir = '/kaggle/working/Dataset-Split'\n\nmain_folders = ['Anorganik', 'Organik']\nsplits = {\n    'Train': 0.8,\n    'Test': 0.1,\n    'Val': 0.1\n}\n\nvalid_extensions = ('.jpg', '.jpeg', '.png')\n\nrandom.seed(42)  \n\ndef create_dir_if_not_exist(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\ndef split_dataset(base_dir, output_dir, main_folders, splits):\n   \n    for split_name in splits.keys():\n        for main_folder in main_folders:\n            create_dir_if_not_exist(os.path.join(output_dir, split_name, main_folder))\n    \n    for main_folder in main_folders:\n        main_path = os.path.join(base_dir, main_folder)\n        if not os.path.isdir(main_path):\n            print(f\"Folder {main_folder} tidak ditemukan, skip.\")\n            continue\n        \n        kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n        \n        for kelas in kelas_list:\n            kelas_path = os.path.join(main_path, kelas)\n            files = [f for f in os.listdir(kelas_path) if f.lower().endswith(valid_extensions)]\n            \n            random.shuffle(files)\n            \n            n_total = len(files)\n            n_train = int(n_total * splits['Train'])\n            n_test = int(n_total * splits['Test'])\n            n_val = n_total - n_train - n_test\n            \n            train_files = files[:n_train]\n            test_files = files[n_train:n_train+n_test]\n            val_files = files[n_train+n_test:]\n            \n            for split_name, file_list in zip(['Train', 'Test', 'Val'], [train_files, test_files, val_files]):\n                kelas_output_path = os.path.join(output_dir, split_name, main_folder, kelas)\n                create_dir_if_not_exist(kelas_output_path)\n                \n                for file_name in file_list:\n                    src_path = os.path.join(kelas_path, file_name)\n                    dst_path = os.path.join(kelas_output_path, file_name)\n                    shutil.copy2(src_path, dst_path)\n            \n            print(f\"{main_folder}/{kelas}: total={n_total}, train={len(train_files)}, test={len(test_files)}, val={len(val_files)}\")\n\nsplit_dataset(working_dir, output_dir, main_folders, splits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:28.091399Z","iopub.execute_input":"2025-05-23T17:55:28.091687Z","iopub.status.idle":"2025-05-23T17:55:31.133186Z","shell.execute_reply.started":"2025-05-23T17:55:28.091667Z","shell.execute_reply":"2025-05-23T17:55:31.132489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dapat dilihat output diatas merupakan hasil split data dengan rasio Train 80, Test 10, dan Val 10","metadata":{}},{"cell_type":"code","source":"# menghitung jumlah gambar train, val, dan test\ndef count_images_in_split(output_dir, main_folders, splits):\n    print(\"\\nTotal jumlah gambar per split:\")\n    for split_name in splits.keys():\n        total_count = 0\n        split_path = os.path.join(output_dir, split_name)\n        for main_folder in main_folders:\n            main_path = os.path.join(split_path, main_folder)\n            if not os.path.isdir(main_path):\n                continue\n            kelas_list = [k for k in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, k))]\n            for kelas in kelas_list:\n                kelas_path = os.path.join(main_path, kelas)\n                files = [f for f in os.listdir(kelas_path) if f.lower().endswith(valid_extensions)]\n                total_count += len(files)\n        print(f\"{split_name}: {total_count} gambar\")\n\ncount_images_in_split(output_dir, main_folders, splits)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:31.135928Z","iopub.execute_input":"2025-05-23T17:55:31.136185Z","iopub.status.idle":"2025-05-23T17:55:31.153369Z","shell.execute_reply.started":"2025-05-23T17:55:31.136168Z","shell.execute_reply":"2025-05-23T17:55:31.152654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Augmentasi, Resize dan Normalisasi untuk folder train\nfrom skimage import exposure\n\n# custom contrast \ndef contrast_stretching(img):\n    p2, p98 = np.percentile(img, (2, 98))\n    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n    return img_rescale\n\n# path folder train setelah split\ntrain_folder = os.path.join(output_dir, 'Train')\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,               # normalisasi\n    rotation_range=30,            # rotasi\n    width_shift_range=0.2,        # translasi horizontal\n    height_shift_range=0.2,       # translasi vertikal\n    shear_range=0.3,              # shearing\n    zoom_range=0.3,               # zoom\n    horizontal_flip=True,         # flip horizontal\n    fill_mode='nearest',\n    channel_shift_range=50,       # perpindahan warna channel\n    ##preprocessing_function=contrast_stretching  # fungsi contrast stretching\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_folder,\n    target_size=(224, 224),  # resize gambar ke ukuran 224x224\n    batch_size=32,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True,\n    seed=42\n)\n\nprint(\"Folder 'train' berhasil diproses dengan ImageDataGenerator.\")\nprint(f\"Jumlah kelas: {len(train_generator.class_indices)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:31.154175Z","iopub.execute_input":"2025-05-23T17:55:31.154476Z","iopub.status.idle":"2025-05-23T17:55:34.778813Z","shell.execute_reply.started":"2025-05-23T17:55:31.154454Z","shell.execute_reply":"2025-05-23T17:55:34.778092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalisasi untuk folder Test dan Validation\ntest_folder = os.path.join(output_dir, 'Test')\nval_folder = os.path.join(output_dir, 'Val')\n\nval_test_datagen = ImageDataGenerator(rescale=1./255)\n\nval_generator = val_test_datagen.flow_from_directory(\n    val_folder,\n    batch_size=32,\n    target_size=(224, 224),  # ini juga harus sama\n    class_mode='categorical',\n    shuffle=False,\n    color_mode= 'rgb'\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    test_folder,\n    batch_size=32,\n    target_size=(224, 224),  # ini juga harus sama\n    class_mode='categorical',\n    shuffle=False,\n    color_mode= 'rgb'\n)\n\n# Output informasi sukses untuk validation\nprint(\"Folder 'validation' berhasil diproses dengan ImageDataGenerator.\")\nprint(f\"Jumlah kelas (val): {len(val_generator.class_indices)}\")\n\n# Output informasi sukses untuk test\nprint(\"Folder 'test' berhasil diproses dengan ImageDataGenerator.\")\nprint(f\"Jumlah kelas (test): {len(test_generator.class_indices)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:34.779564Z","iopub.execute_input":"2025-05-23T17:55:34.779802Z","iopub.status.idle":"2025-05-23T17:55:34.816412Z","shell.execute_reply.started":"2025-05-23T17:55:34.779786Z","shell.execute_reply":"2025-05-23T17:55:34.815705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Mapping kelas: {train_generator.class_indices}\")\nprint(f\"Mapping kelas (val): {val_generator.class_indices}\")\nprint(f\"Mapping kelas (test): {test_generator.class_indices}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:34.817034Z","iopub.execute_input":"2025-05-23T17:55:34.817284Z","iopub.status.idle":"2025-05-23T17:55:34.821618Z","shell.execute_reply.started":"2025-05-23T17:55:34.817267Z","shell.execute_reply":"2025-05-23T17:55:34.820908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Modelling**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\n# Menghitung jumlah data per label\nlabel_counts = Counter(train_generator.classes)\n\n# Mengambil mapping label\nlabel_map = train_generator.class_indices\nlabel_map_inv = {v: k for k, v in label_map.items()}\n\n# Menampilkan hasil\nfor label_index, count in label_counts.items():\n    print(f\"{label_map_inv[label_index]}: {count} gambar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:34.822527Z","iopub.execute_input":"2025-05-23T17:55:34.823262Z","iopub.status.idle":"2025-05-23T17:55:34.834998Z","shell.execute_reply.started":"2025-05-23T17:55:34.823239Z","shell.execute_reply":"2025-05-23T17:55:34.834193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Ambil semua label dari generator\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(\"Class weights:\", class_weights_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:34.835717Z","iopub.execute_input":"2025-05-23T17:55:34.835892Z","iopub.status.idle":"2025-05-23T17:55:34.849138Z","shell.execute_reply.started":"2025-05-23T17:55:34.835879Z","shell.execute_reply":"2025-05-23T17:55:34.848352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNetV2\n\n# Input layer\ninput_tensor = Input(shape=(224, 224, 3))\n\n# Base model\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\nprint(\"Jumlah total layer:\", len(base_model.layers))\n\n# Set semua layer dapat dilatih terlebih dahulu\nbase_model.trainable = True\n\n# Bekukan semua layer kecuali 50 terakhir\nfor layer in base_model.layers[:-50]:\n    layer.trainable = False\n\n# Tambahkan custom layers di atas base model\nx = base_model.output\nx = Conv2D(8, (3,3), activation='relu')(x)\nx = BatchNormalization()(x)\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\noutput_tensor = Dense(len(train_generator.class_indices), activation='softmax')(x)\n\n# Buat model final\nmodel = Model(inputs=input_tensor, outputs=output_tensor)\n\n# Lihat arsitektur model\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:34.850138Z","iopub.execute_input":"2025-05-23T17:55:34.850423Z","iopub.status.idle":"2025-05-23T17:55:35.777760Z","shell.execute_reply.started":"2025-05-23T17:55:34.850390Z","shell.execute_reply":"2025-05-23T17:55:35.777007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Callback ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3,\n    verbose=1,\n    min_lr=1e-6\n)\n\n# Callback EarlyStopping\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=7,             # jika val_loss tidak membaik selama 5 epoch, training stop\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Compile dan fit dengan epochs 50 dan callbacks\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nhistory1 = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=50,\n    callbacks=[reduce_lr, early_stop],\n    class_weight=class_weights_dict if class_weights_dict else None\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:35.778692Z","iopub.execute_input":"2025-05-23T17:55:35.778880Z","iopub.status.idle":"2025-05-23T19:02:38.923556Z","shell.execute_reply.started":"2025-05-23T17:55:35.778865Z","shell.execute_reply":"2025-05-23T19:02:38.922780Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Evaluasi Model dan Visualisasi**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport numpy as np\n\n# ===== 1. VISUALISASI =====\n# Menggabungkan history1 dan history2 jika kamu mau, tapi di sini hanya history2\nacc = history1.history['accuracy']\nval_acc = history1.history['val_accuracy']\nloss = history1.history['loss']\nval_loss = history1.history['val_loss']\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(14, 5))\n\n# Plot Akurasi\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training vs Validation Accuracy')\n\n# Plot Loss\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training vs Validation Loss')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:03:49.427454Z","iopub.execute_input":"2025-05-23T19:03:49.428061Z","iopub.status.idle":"2025-05-23T19:03:49.748773Z","shell.execute_reply.started":"2025-05-23T19:03:49.428016Z","shell.execute_reply":"2025-05-23T19:03:49.748099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ambil prediksi model\ny_pred_probs = model.predict(test_generator)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = test_generator.classes\nclass_labels = list(test_generator.class_indices.keys())\n\n# Classification Report\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_true, y_pred, target_names=class_labels))\n\n# Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_labels,\n            yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:03:52.288489Z","iopub.execute_input":"2025-05-23T19:03:52.288758Z","iopub.status.idle":"2025-05-23T19:03:58.479017Z","shell.execute_reply.started":"2025-05-23T19:03:52.288732Z","shell.execute_reply":"2025-05-23T19:03:58.478164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5. Konversi Model**","metadata":{}},{"cell_type":"code","source":"model.save(\"model_organik_anorganik.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:05:51.646398Z","iopub.execute_input":"2025-05-23T19:05:51.647083Z","iopub.status.idle":"2025-05-23T19:05:52.079129Z","shell.execute_reply.started":"2025-05-23T19:05:51.647056Z","shell.execute_reply":"2025-05-23T19:05:52.078307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('model_organik_anorganik.h5')  # klik link yang muncul untuk download","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:05:54.032843Z","iopub.execute_input":"2025-05-23T19:05:54.033477Z","iopub.status.idle":"2025-05-23T19:05:54.039301Z","shell.execute_reply.started":"2025-05-23T19:05:54.033447Z","shell.execute_reply":"2025-05-23T19:05:54.038518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tf2onnx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:07:30.099490Z","iopub.execute_input":"2025-05-23T19:07:30.100280Z","iopub.status.idle":"2025-05-23T19:07:33.314176Z","shell.execute_reply.started":"2025-05-23T19:07:30.100256Z","shell.execute_reply":"2025-05-23T19:07:33.313247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tf2onnx\nimport tensorflow as tf\n\n# Konversi model ke ONNX\nspec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\nonnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n\n# Simpan model ONNX\nwith open(\"model_organik_anorganik.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:07:35.760293Z","iopub.execute_input":"2025-05-23T19:07:35.761268Z","iopub.status.idle":"2025-05-23T19:07:43.188828Z","shell.execute_reply.started":"2025-05-23T19:07:35.761230Z","shell.execute_reply":"2025-05-23T19:07:43.188022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('model_organik_anorganik.onnx')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:07:51.288381Z","iopub.execute_input":"2025-05-23T19:07:51.288830Z","iopub.status.idle":"2025-05-23T19:07:51.294336Z","shell.execute_reply.started":"2025-05-23T19:07:51.288805Z","shell.execute_reply":"2025-05-23T19:07:51.293425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflowjs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:08:18.057663Z","iopub.execute_input":"2025-05-23T19:08:18.057938Z","iopub.status.idle":"2025-05-23T19:08:21.581055Z","shell.execute_reply.started":"2025-05-23T19:08:18.057919Z","shell.execute_reply":"2025-05-23T19:08:21.580068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **6. Inference**","metadata":{}}]}